{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "dataset = np.insert(iris.data, 0, iris.target,axis=1)\n",
    "\n",
    "df = pd.DataFrame(data=dataset, columns=['iris_id'] + iris.feature_names)\n",
    "df['species'] = df['iris_id'].map(lambda x: 'setosa' if x == 0 else 'versicolor' if x == 1 else 'virginica')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "yX_train = np.column_stack((y_train, X_train))\n",
    "yX_test = np.column_stack((y_test, X_test))\n",
    "np.savetxt(\"iris_train.csv\", yX_train, delimiter=\",\", fmt='%0.3f')\n",
    "np.savetxt(\"iris_test.csv\", yX_test, delimiter=\",\", fmt='%0.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "prefix='mlops/iris'\n",
    "# Retrieve the default bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "assert(sagemaker.__version__ >= \"2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset to an S3 bucket\n",
    "input_train = sagemaker_session.upload_data(path='iris_train.csv', key_prefix='%s/data' % prefix)\n",
    "input_test = sagemaker_session.upload_data(path='iris_test.csv', key_prefix='%s/data' % prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(s3_data=input_train,content_type=\"csv\")\n",
    "test_data = sagemaker.inputs.TrainingInput(s3_data=input_test,content_type=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the URI for new container\n",
    "container_uri = sagemaker.image_uris.retrieve('xgboost', boto3.Session().region_name, version='1.0-1')\n",
    "\n",
    "# Create the estimator\n",
    "xgb = sagemaker.estimator.Estimator(container_uri,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "# Set the hyperparameters\n",
    "xgb.set_hyperparameters(eta=0.1,\n",
    "                        max_depth=5,\n",
    "                        gamma=4,\n",
    "                        num_class=len(np.unique(y)),\n",
    "                        alpha=10,\n",
    "                        min_child_weight=6,\n",
    "                        silent=0,\n",
    "                        objective='multi:softmax',\n",
    "                        num_round=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# takes around 3min 11s\n",
    "xgb.fit({'train': train_data, 'validation': test_data, })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Enable log capturing in the endpoint\n",
    "data_capture_configuration = sagemaker.model_monitor.data_capture_config.DataCaptureConfig(\n",
    "    enable_capture=True, \n",
    "    sampling_percentage=100, \n",
    "    destination_s3_uri='s3://{}/{}/monitoring'.format(bucket, prefix), \n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "xgb_predictor = xgb.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m4.xlarge',\n",
    "    data_capture_config=data_capture_configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sklearn.metrics import f1_score\n",
    "csv_serializer = CSVSerializer()\n",
    "\n",
    "endpoint_name = xgb_predictor.endpoint_name\n",
    "model_name = boto3.client('sagemaker').describe_endpoint_config(\n",
    "    EndpointConfigName=endpoint_name\n",
    ")['ProductionVariants'][0]['ModelName']\n",
    "!echo $model_name > model_name.txt\n",
    "!echo $endpoint_name > endpoint_name.txt\n",
    "xgb_predictor.serializer = csv_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = [ float(xgb_predictor.predict(x).decode('utf-8')) for x in X_test] \n",
    "score = f1_score(y_test,predictions_test,labels=[0.0,1.0,2.0],average='micro')\n",
    "\n",
    "print('F1 Score(micro): %.1f' % (score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "csv_serializer = CSVSerializer()\n",
    "\n",
    "sm = boto3.client('sagemaker-runtime')\n",
    "resp = sm.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType='text/csv',\n",
    "    Body=csv_serializer.serialize(X_test[0])\n",
    ")\n",
    "prediction = float(resp['Body'].read().decode('utf-8'))\n",
    "print('Predicted class: %.1f for [%s]' % (prediction, csv_serializer.serialize(X_test[0])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "role = get_execution_role()\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "dataset = np.insert(X, 0, y,axis=1)\n",
    "pd.DataFrame(data=dataset, columns=['iris_id'] + iris.feature_names).to_csv('full_dataset.csv', index=None)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix='mlops/iris'\n",
    "endpoint_name = open('endpoint_name.txt', 'r').read().strip() if os.path.isfile('endpoint_name.txt') else None\n",
    "endpoint_name2 = open('endpoint_name2.txt', 'r').read().strip() if os.path.isfile('endpoint_name2.txt') else None\n",
    "\n",
    "try:\n",
    "    xgb_predictor = sagemaker.predictor.Predictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "    xgb_predictor.serializer = CSVSerializer()\n",
    "except Exception as e:\n",
    "    raise Exception(\"You must run Part 1 before this. There, you will train/deploy a Model and use it here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "endpoint_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "endpoint_monitor.suggest_baseline(\n",
    "    baseline_dataset='full_dataset.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri='s3://{}/{}/monitoring/baseline'.format(bucket, prefix),\n",
    "    wait=True,\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = endpoint_monitor.latest_baselining_job\n",
    "schema_df = pd.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "constraints_df = pd.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "report_df = schema_df.merge(constraints_df)\n",
    "report_df.drop([\n",
    "    'numerical_statistics.distribution.kll.buckets',\n",
    "    'numerical_statistics.distribution.kll.sketch.data',\n",
    "    'numerical_statistics.distribution.kll.sketch.parameters.c'\n",
    "], axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_monitor.create_monitoring_schedule(\n",
    "    endpoint_input=endpoint_name,\n",
    "    output_s3_uri='s3://{}/{}/monitoring/reports'.format(bucket, prefix),\n",
    "    statistics=endpoint_monitor.baseline_statistics(),\n",
    "    constraints=endpoint_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you can list all the monitoring schedules you created in your account\n",
    "!aws sagemaker list-monitoring-schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time \n",
    "from threading import Thread\n",
    "\n",
    "traffic_generator_running=True\n",
    "def invoke_endpoint_forever():\n",
    "    print('Invoking endpoint forever!')\n",
    "    while traffic_generator_running:\n",
    "        ## This will create an invalid set of features\n",
    "        ## The idea is to violate two monitoring constraings: not_null and data_drift\n",
    "        null_idx = random.randint(0,3)\n",
    "        sample = [random.randint(500,2000) / 100.0 for i in range(4)]\n",
    "        sample[null_idx] = None\n",
    "        xgb_predictor.predict(sample)\n",
    "        time.sleep(0.5)\n",
    "    print('Endpoint invoker has stopped')\n",
    "Thread(target = invoke_endpoint_forever).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import boto3\n",
    "\n",
    "def process_monitoring_logs(endpoint_monitor):\n",
    "    sm = boto3.client('sagemaker')\n",
    "    now = datetime.datetime.today()\n",
    "    suffix = now.strftime(\"%Y/%m/%d/%H\")\n",
    "    start_time = datetime.datetime(now.year, now.month, now.day, now.hour)\n",
    "    end_time = start_time + datetime.timedelta(hours=1)\n",
    "\n",
    "    # get the monitoring metadata\n",
    "    base_desc = endpoint_monitor.describe_latest_baselining_job()\n",
    "    sche_desc = endpoint_monitor.describe_schedule()\n",
    "    baseline_path = base_desc['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']\n",
    "    endpoint_name = sche_desc['EndpointName']\n",
    "\n",
    "    variant_name = sm.describe_endpoint(EndpointName=endpoint_name)['ProductionVariants'][0]['VariantName']\n",
    "    logs_path = \"%s/%s/%s\" % (endpoint_name,variant_name,suffix)\n",
    "    \n",
    "    s3_output = {\n",
    "        \"S3Uri\": 's3://{}/{}/monitoring/{}'.format(bucket, prefix, logs_path),\n",
    "        \"LocalPath\": \"/opt/ml/processing/output\",\n",
    "        \"S3UploadMode\": \"Continuous\"\n",
    "    }\n",
    "    # values for the processing job input\n",
    "    values = [\n",
    "        [ 'input_1', 's3://{}/{}/monitoring/{}'.format(bucket, prefix, logs_path),\n",
    "            '/opt/ml/processing/input/endpoint/{}'.format(logs_path) ], \n",
    "        [ 'baseline', '%s/statistics.json' % baseline_path,\n",
    "            '/opt/ml/processing/baseline/stats'],\n",
    "        [ 'constraints', '%s/constraints.json' % baseline_path,\n",
    "            '/opt/ml/processing/baseline/constraints']\n",
    "    ]\n",
    "    job_params = {\n",
    "        'ProcessingJobName': 'model-monitoring-%s' % time.strftime(\"%Y%m%d%H%M%S\"),\n",
    "        'ProcessingInputs': [{\n",
    "            'InputName': o[0],\n",
    "            'S3Input': { \n",
    "                'S3Uri': o[1], 'LocalPath': o[2], 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', \n",
    "                'S3CompressionType': 'None', 'S3DataDistributionType': 'FullyReplicated'\n",
    "            }} for o in values],\n",
    "        'ProcessingOutputConfig': { 'Outputs': [ {'OutputName': 'result','S3Output': s3_output } ] },\n",
    "        'ProcessingResources': base_desc['ProcessingResources'],\n",
    "        'AppSpecification': base_desc['AppSpecification'],\n",
    "        'RoleArn': base_desc['RoleArn'],\n",
    "        'Environment': {\n",
    "            'baseline_constraints': '/opt/ml/processing/baseline/constraints/constraints.json',\n",
    "            'baseline_statistics': '/opt/ml/processing/baseline/stats/statistics.json',\n",
    "            'dataset_format': '{\"sagemakerCaptureJson\":{\"captureIndexNames\":[\"endpointInput\",\"endpointOutput\"]}}',\n",
    "            'dataset_source': '/opt/ml/processing/input/endpoint',      \n",
    "            'output_path': '/opt/ml/processing/output',\n",
    "            'publish_cloudwatch_metrics': 'Enabled',\n",
    "            'sagemaker_monitoring_schedule_name': sche_desc['MonitoringScheduleName'],\n",
    "            'sagemaker_endpoint_name': endpoint_name,\n",
    "            'start_time': start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            'end_time': end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        }\n",
    "    }\n",
    "    sm.create_processing_job(**job_params)\n",
    "    waiter = sm.get_waiter('processing_job_completed_or_stopped')\n",
    "    waiter.wait( ProcessingJobName=job_params['ProcessingJobName'], WaiterConfig={'Delay': 30,'MaxAttempts': 20} )\n",
    "    return job_params['ProcessingJobName'], s3_output['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## The processing job takes something like 5mins to run\n",
    "job_name, s3_output = process_monitoring_logs(endpoint_monitor)\n",
    "tokens = s3_output.split('/', 3)\n",
    "df = pd.read_json(sagemaker_session.read_s3_file(tokens[2], '%s/constraint_violations.json' % tokens[3]))\n",
    "df = pd.json_normalize(df.violations)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_generator_running=False\n",
    "time.sleep(3)\n",
    "endpoint_monitor.delete_monitoring_schedule()\n",
    "time.sleep(10) # wait for 10 seconds before trying to delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    xgb_predictor = sagemaker.predictor.Predictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "    xgb_predictor.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "try:\n",
    "    xgb_predictor2 = sagemaker.predictor.Predictor(endpoint_name=endpoint_name2, sagemaker_session=sagemaker_session)\n",
    "    xgb_predictor2.delete_endpoint()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Hourly Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'C:/Users/mohil/Desktop/Projects/Data/MLOps/iris-mlops/amazon-sagemaker-mlops-workshop-master/Monitoring_Reports_Hourly/22-06-04-15-37/constraints.json'\n",
    "path2 = 'C:/Users/mohil/Desktop/Projects/Data/MLOps/iris-mlops/amazon-sagemaker-mlops-workshop-master/Monitoring_Reports_Hourly/22-06-04-15-37/statistics.json'\n",
    "path3 = 'C:/Users/mohil/Desktop/Projects/Data/MLOps/iris-mlops/amazon-sagemaker-mlops-workshop-master/Monitoring_Reports_Hourly/22-06-04-15-37/constraint_violations.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
